{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "ticker = \"AAPL\"\n",
    "start_date = \"2020-01-01\"\n",
    "end_date =\"2022-01-01\"\n",
    "\n",
    "\n",
    "stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data.to_pickle('test.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 505 entries, 2020-01-02 to 2021-12-31\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Open       505 non-null    float64\n",
      " 1   High       505 non-null    float64\n",
      " 2   Low        505 non-null    float64\n",
      " 3   Close      505 non-null    float64\n",
      " 4   Adj Close  505 non-null    float64\n",
      " 5   Volume     505 non-null    int64  \n",
      "dtypes: float64(5), int64(1)\n",
      "memory usage: 27.6 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(stock_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open         0\n",
      "High         0\n",
      "Low          0\n",
      "Close        0\n",
      "Adj Close    0\n",
      "Volume       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(stock_data.isnull().sum())\n",
    "\n",
    "stock_data['Daily_return']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate daily returns\n",
    "stock_data['Daily_Return'] = stock_data['Adj Close'].pct_change()\n",
    "\n",
    "# Add moving averages\n",
    "stock_data['MA_50'] = stock_data['Adj Close'].rolling(window=50).mean()\n",
    "stock_data['MA_200'] = stock_data['Adj Close'].rolling(window=200).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant features\n",
    "features = ['Open', 'High', 'Low', 'Close', 'Volume', 'Daily_Return', 'MA_50', 'MA_200']\n",
    "input_data = stock_data[features]\n",
    "\n",
    "num_features = 8\n",
    "\n",
    "# Normalize data if needed (optional)\n",
    "normalized_data = (input_data - input_data.mean()) / input_data.std()\n",
    "\n",
    "# Define sequence length (e.g., 30 days)\n",
    "sequence_length = 30\n",
    "\n",
    "# Create input sequences\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(len(normalized_data) - sequence_length):\n",
    "    X.append(normalized_data.iloc[i:i + sequence_length].values)\n",
    "    y.append(normalized_data.iloc[i + sequence_length]['Close'])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(X) * split_ratio)\n",
    "\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming X_train and X_test are your input sequences\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.reshape(-1, num_features)).reshape(X_train.shape)\n",
    "X_test_scaled = scaler.transform(X_test.reshape(-1, num_features)).reshape(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"time_distributed\" (type TimeDistributed).\n\nInput 0 of layer \"conv1d_11\" is incompatible with the layer: expected min_ndim=3, found ndim=2. Full shape received: (None, 8)\n\nCall arguments received by layer \"time_distributed\" (type TimeDistributed):\n  • inputs=tf.Tensor(shape=(None, 30, 8), dtype=float32)\n  • training=None\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 72\u001b[0m\n\u001b[0;32m     69\u001b[0m num_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Build and compile the model\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mff_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 41\u001b[0m, in \u001b[0;36mbuild_model\u001b[1;34m(sequence_length, num_heads, ff_dim, dropout, mlp_units)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m units \u001b[38;5;129;01min\u001b[39;00m mlp_units:\n\u001b[0;32m     39\u001b[0m     x \u001b[38;5;241m=\u001b[39m transformer_encoder(x, num_features, num_heads, ff_dim, dropout)\n\u001b[1;32m---> 41\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTimeDistributed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv1D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mModel(inputs\u001b[38;5;241m=\u001b[39minputs, outputs\u001b[38;5;241m=\u001b[39moutputs)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\Rober\\.virtualenvs\\AASD4010-XSwstjEU\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Rober\\.virtualenvs\\AASD4010-XSwstjEU\\lib\\site-packages\\keras\\src\\engine\\input_spec.py:253\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    251\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m ndim \u001b[38;5;241m<\u001b[39m spec\u001b[38;5;241m.\u001b[39mmin_ndim:\n\u001b[1;32m--> 253\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    254\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    255\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis incompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    256\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected min_ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mmin_ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    257\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    258\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    259\u001b[0m         )\n\u001b[0;32m    260\u001b[0m \u001b[38;5;66;03m# Check dtype.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"time_distributed\" (type TimeDistributed).\n\nInput 0 of layer \"conv1d_11\" is incompatible with the layer: expected min_ndim=3, found ndim=2. Full shape received: (None, 8)\n\nCall arguments received by layer \"time_distributed\" (type TimeDistributed):\n  • inputs=tf.Tensor(shape=(None, 30, 8), dtype=float32)\n  • training=None\n  • mask=None"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "# Assuming 'X_train' and 'y_train' are your input features and target variables\n",
    "\n",
    "num_features = 8\n",
    "\n",
    "# Positional encoding layer\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, sequence_length, output_dim):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(sequence_length, output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(PositionalEncoding, self).get_config()\n",
    "        return config\n",
    "\n",
    "    def positional_encoding(self, sequence_length, output_dim):\n",
    "        angle_rads = self.get_angles(np.arange(sequence_length)[:, np.newaxis], np.arange(output_dim)[np.newaxis, :], output_dim)\n",
    "        sines = np.sin(angle_rads[:, 0::2])\n",
    "        cosines = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[np.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def get_angles(self, positions, i, d_model):\n",
    "        angles = 1 / np.power(10000.0, (2 * (i // 2)) / np.float32(d_model))\n",
    "        return positions * angles\n",
    "\n",
    "def build_model(sequence_length, num_heads, ff_dim, dropout=0, mlp_units=[128]):\n",
    "    inputs = keras.Input(shape=(sequence_length, num_features))  # Assuming num_features is the number of features in your input data\n",
    "    x = PositionalEncoding(sequence_length, num_features)(inputs)\n",
    "    \n",
    "    for units in mlp_units:\n",
    "        x = transformer_encoder(x, num_features, num_heads, ff_dim, dropout)\n",
    "    \n",
    "    outputs = layers.TimeDistributed(layers.Conv1D(filters=num_features, kernel_size=1, activation=\"linear\"))(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Transformer model\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res\n",
    "\n",
    "# Build the Transformer model\n",
    "\n",
    "\n",
    "# Define hyperparameters\n",
    "sequence_length = 30\n",
    "num_heads = 2\n",
    "ff_dim = 30\n",
    "dropout = 0.25\n",
    "num_features = 8\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_model(sequence_length, num_heads, ff_dim, dropout)\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "10/10 [==============================] - 1s 15ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: nan - val_loss: nan\n",
      "Epoch 11/50\n",
      "10/10 [==============================] - 0s 8ms/step - loss: nan - val_loss: nan\n",
      "Epoch 12/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: nan - val_loss: nan\n",
      "Epoch 13/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: nan - val_loss: nan\n",
      "Epoch 14/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: nan - val_loss: nan\n",
      "Epoch 15/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 16/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 17/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 18/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 19/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 20/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 21/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 22/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 23/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 24/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: nan - val_loss: nan\n",
      "Epoch 25/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 26/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 27/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 28/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 29/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 30/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 31/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 32/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 33/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 34/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 35/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 36/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 37/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 38/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 39/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 40/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 41/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 42/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 43/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 44/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 45/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 46/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 47/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 48/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 49/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 50/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: nan - val_loss: nan\n",
      "3/3 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def build_transformer_model(sequence_length, num_features, num_heads, ff_dim, dropout_rate):\n",
    "    inputs = keras.Input(shape=(sequence_length, num_features))\n",
    "    x = layers.MultiHeadAttention(num_heads=num_heads, key_dim=num_features)(inputs, inputs)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Conv1D(filters=num_features, kernel_size=1)(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Define hyperparameters\n",
    "sequence_length = 30\n",
    "num_features = 8\n",
    "num_heads = 2\n",
    "ff_dim = 32\n",
    "dropout_rate = 0.2\n",
    "\n",
    "# Build the transformer model\n",
    "model = build_transformer_model(sequence_length, num_features, num_heads, ff_dim, dropout_rate)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MultiHeadAttention.call() missing 1 required positional argument: 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Build the model\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_transformer_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mff_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[0;32m     36\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[23], line 21\u001b[0m, in \u001b[0;36mbuild_transformer_model\u001b[1;34m(sequence_length, num_features, num_heads, ff_dim, dropout_rate)\u001b[0m\n\u001b[0;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[0;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Input(shape\u001b[38;5;241m=\u001b[39m(sequence_length, num_features)))\n\u001b[1;32m---> 21\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMultiHeadAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dropout(dropout_rate))\n\u001b[0;32m     23\u001b[0m model\u001b[38;5;241m.\u001b[39madd(LayerNormalization(epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Rober\\.virtualenvs\\AASD4010-XSwstjEU\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:204\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 204\u001b[0m   result \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Rober\\.virtualenvs\\AASD4010-XSwstjEU\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Rober\\.virtualenvs\\AASD4010-XSwstjEU\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    456\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m\u001b[38;5;241m.\u001b[39mcall(args, kwargs)\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 459\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n",
      "\u001b[1;31mTypeError\u001b[0m: MultiHeadAttention.call() missing 1 required positional argument: 'value'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, MultiHeadAttention, Dropout, LayerNormalization, Conv1D, GlobalAveragePooling1D, Input\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "# Assuming X_train, y_train, X_test are your input sequences and target values\n",
    "# Reshape X_train and X_test to 3D (num_samples, sequence_length, num_features)\n",
    "X_train_reshaped = X_train.reshape(X_train.shape[0], sequence_length, num_features)\n",
    "X_test_reshaped = X_test.reshape(X_test.shape[0], sequence_length, num_features)\n",
    "\n",
    "# Standardize input features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_reshaped.reshape(-1, num_features)).reshape(X_train_reshaped.shape)\n",
    "X_test_scaled = scaler.transform(X_test_reshaped.reshape(-1, num_features)).reshape(X_test_reshaped.shape)\n",
    "\n",
    "# Define the model\n",
    "def build_transformer_model(sequence_length, num_features, num_heads, ff_dim, dropout_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(sequence_length, num_features)))\n",
    "    model.add(MultiHeadAttention(num_heads=num_heads, key_dim=num_features))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LayerNormalization(epsilon=1e-6))\n",
    "    model.add(Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\"))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Conv1D(filters=num_features, kernel_size=1))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "model = build_transformer_model(sequence_length, num_features, num_heads, ff_dim, dropout_rate)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
    "\n",
    "# Define a callback to print loss during training\n",
    "class PrintLossCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"Epoch {epoch+1}/{self.params['epochs']}, Loss: {logs['loss']}, Val Loss: {logs['val_loss']}\")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[PrintLossCallback()])\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test_scaled)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AASD4010-XSwstjEU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
